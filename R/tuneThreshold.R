#' @title Tune prediction threshold.
#'
#' @description
#' Optimizes the threshold of predictions based on probabilities.
#' Works for classification and multilabel tasks.
#' Uses \code{\link[BBmisc]{optimizeSubInts}} for normal binary class problems and \code{\link[cmaes]{cma_es}}
#' for multiclass and multilabel problems.
#'
#' @template arg_pred
#' @param measure [\code{\link{Measure}}]\cr
#'   Performance measure to optimize.
#'   Default is the default measure for the task.
#' @param task [\code{\link{Task}}]\cr
#'   Learning task. Rarely neeeded,
#'   only when required for the performance measure.
#' @param model [\code{\link{WrappedModel}}]\cr
#'   Fitted model. Rarely neeeded,
#'   only when required for the performance measure.
#' @param nsub [\code{integer(1)}]\cr
#'   Passed to \code{\link[BBmisc]{optimizeSubInts}} for 2class problems.
#'   Default is 20. For AMV performance measure it is always 1 due to computational constraints.
#' @param control [\code{list}]\cr
#'   Control object for \code{\link[cmaes]{cma_es}} when used.
#'   Default is empty list.
#' @return [\code{list}]. A named list with with the following components:
#'   \code{th} is the optimal threshold, \code{perf} the performance value.
#' @family tune
#' @export
tuneThreshold = function(pred, measure, task, model, nsub = 20L, control = list()) {
  checkPrediction(pred, task.type = c("oneclass", "classif", "multilabel"), predict.type = "prob")
  td = pred$task.desc
  ttype = td$type
  measure = checkMeasures(measure, td)[[1L]]
  if (!missing(task))
    assertClass(task, classes = "SupervisedTask")
  if (!missing(model) && !is.list(model))
    assertClass(model, classes = "WrappedModel")
  if (!missing(model) && any(class(pred) %in% "ResamplePrediction")) {
    assertClass(model, classes = "list")
    for (i in model) assertClass(i, classes = "WrappedModel")
  }
  assertList(control)

  probs = getPredictionProbabilities(pred)

  # brutally return NA if we find any NA in the predicted probs...
  if (anyMissing(probs)) {
    return(list(th = NA, pred = pred, th.seq = numeric(0), perf = numeric(0)))
  }

  cls = pred$task.desc$class.levels
  k = length(cls)

  fitn = function(x) {
    if (ttype == "multilabel" || k > 2)
      names(x) = cls

    # If setthreshold is applied on a Resampleprediction object, the variable models
    # has as many models as resample iters, amv-performance need a model to evaluate the pred
    # pred has the prediction of the whole data set, it consist of iters parts (= #iters of resample)
    # each part was the test set of a resample iters.
    if (any(class(pred) %in% "ResamplePrediction" && grepl("AMV", measure$id))) {
      y.tmp = vector()
      for (i in seq_along(model)) {
        pred.tmp = setThreshold(pred, threshold = x)
        pred.tmp$data = pred.tmp$data[pred.tmp$data$iter == i, ]
        y.tmp[i] = performance(pred.tmp, measures = measure, model = model[[i]], task = task)
      }
      y = mean(y.tmp)
      names(y) = measure$id
    } else {
      y = performance(setThreshold(pred, x), measure, task, model)
    }
    return(y)
  }

  if (ttype == "multilabel" || k > 2L) {
    requirePackages("cmaes", why = "tuneThreshold", default.method = "load")
    start = rep(0.5, k)
    or = cmaes::cma_es(par = start, fn = fitn, lower = 0, upper = 1, control = control)
    th = or$par / sum(or$par)
    names(th) = cls
    perf = or$val
  } else { # classif with k = 2
    if ( (pred$task.desc$type == "oneclass" & abs(diff(range(probs))) < 1) | grepl("AMV", measure$id)) {
      or = optimizeSubInts(f = fitn, lower = min(probs)-0.001, upper = max(probs)+0.001, maximum = !measure$minimize, nsub = 1)
    } else {
      or = optimizeSubInts(f = fitn, lower = 0, upper = 1, maximum = !measure$minimize, nsub = nsub)
    }
    th = or[[1]]
    perf = or$objective
  }
  return(list(th = th, perf = perf))
}
